{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CVPkafsq1sAX",
        "outputId": "b6426fa0-f82f-4c4a-efb6-a4b873711181"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1)\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.10/dist-packages (0.15.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.101)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.10/dist-packages (from torch) (10.2.10.91)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.0.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.4.91)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.10/dist-packages (from torch) (2.14.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.91)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (75.1.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.44.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.30.5)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (18.1.8)\n",
            "Requirement already satisfied: torchdata==0.6.1 in /usr/local/lib/python3.10/dist-packages (from torchtext) (0.6.1)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.6.1->torchtext) (2.2.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "%pip install transformers torch torchtext scikit-learn pandas\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install torch==2.0.1 torchtext==0.15.2\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6yv9EL44xpU",
        "outputId": "c19ecd44-bd45-4c7f-c66d-fe559de8a92f"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.10/dist-packages (2.0.1)\n",
            "Requirement already satisfied: torchtext==0.15.2 in /usr/local/lib/python3.10/dist-packages (0.15.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (11.7.101)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (10.2.10.91)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (11.4.0.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (11.7.4.91)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (2.14.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (11.7.91)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (2.0.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.15.2) (4.66.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.15.2) (2.32.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.15.2) (1.26.4)\n",
            "Requirement already satisfied: torchdata==0.6.1 in /usr/local/lib/python3.10/dist-packages (from torchtext==0.15.2) (0.6.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (75.1.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (0.44.0)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.6.1->torchtext==0.15.2) (2.2.3)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1) (3.30.5)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1) (18.1.8)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.15.2) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.15.2) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.15.2) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from transformers import AdamW\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "data = [\n",
        "    {\"crimeaditionalinfo\": \"I have continued receiving random calls and abusive messages on WhatsApp. Someone added my number to an unknown Facebook group called 'Only Girls' and I am still getting calls from unknown numbers. Please help.\", \"category\": \"Harassment\"},\n",
        "    {\"crimeaditionalinfo\": \"The fraudster is continuously messaging me, asking me to pay him money or he will send fake nude photos of me to my contacts through WhatsApp.\", \"category\": \"Fraud\"},\n",
        "    {\"crimeaditionalinfo\": \"He is pretending to be a police officer and demanding money with legal-sounding threats in text messages.\", \"category\": \"Harassment\"},\n",
        "    {\"crimeaditionalinfo\": \"I applied for a telecalling job and paid security fees, but the job didn’t match the description. Please help recover my financial loss.\", \"category\": \"Scam\"},\n",
        "    {\"crimeaditionalinfo\": \"Received a call from someone asking for OTP verification, claiming to be from my bank. I did not provide it, but I suspect it was an attempt to steal my information.\", \"category\": \"Phishing\"},\n",
        "    {\"crimeaditionalinfo\": \"An app on the Play Store accessed my contacts without permission and is now harassing me to repay a loan I never took.\", \"category\": \"Harassment\"},\n",
        "    {\"crimeaditionalinfo\": \"Received a message from someone pretending to be a friend asking for an urgent loan. Later found out it wasn’t my friend.\", \"category\": \"Identity Theft\"},\n",
        "    {\"crimeaditionalinfo\": \"Someone created a fake social media profile in my name and has been messaging people with offensive content.\", \"category\": \"Identity Theft\"},\n",
        "    {\"crimeaditionalinfo\": \"A company promised an investment opportunity with high returns, but after I invested, they stopped responding.\", \"category\": \"Fraud\"},\n",
        "    {\"crimeaditionalinfo\": \"Received multiple calls asking me to update KYC for my bank account, requesting personal details. It seemed suspicious.\", \"category\": \"Phishing\"},\n",
        "    {\"crimeaditionalinfo\": \"A website claimed to sell electronics at a discount, but after placing an order, there was no response from them.\", \"category\": \"Scam\"},\n",
        "    {\"crimeaditionalinfo\": \"Received an email claiming I won a prize and asking for payment to cover taxes. I suspect it's a scam.\", \"category\": \"Scam\"},\n",
        "    {\"crimeaditionalinfo\": \"Someone called, pretending to be a government official and threatened legal action unless I paid a fee.\", \"category\": \"Fraud\"},\n",
        "    {\"crimeaditionalinfo\": \"An unknown person has been sending harassing messages and calling my phone, affecting my personal life.\", \"category\": \"Harassment\"},\n",
        "    {\"crimeaditionalinfo\": \"I was contacted by someone claiming to represent my bank, asking for my account details to prevent account suspension.\", \"category\": \"Phishing\"},\n",
        "    {\"crimeaditionalinfo\": \"A man I met online asked for emergency money. He claimed to be in a difficult situation, but I believe it was a scam.\", \"category\": \"Scam\"},\n",
        "    {\"crimeaditionalinfo\": \"Someone used my identity to apply for a loan and is now harassing my contacts for repayment.\", \"category\": \"Identity Theft\"},\n",
        "    {\"crimeaditionalinfo\": \"Received a call from a 'representative' of a tech company, asking for remote access to my computer to fix an issue.\", \"category\": \"Phishing\"},\n",
        "    {\"crimeaditionalinfo\": \"I received multiple calls and messages threatening me to repay a loan I never took. Please help.\", \"category\": \"Harassment\"},\n",
        "    {\"crimeaditionalinfo\": \"Someone used my profile picture and created a fake account to impersonate me on social media.\", \"category\": \"Identity Theft\"},\n",
        "    {\"crimeaditionalinfo\": \"An online store claimed to have a limited-time offer, but after paying, I didn’t receive the product.\", \"category\": \"Scam\"},\n",
        "    {\"crimeaditionalinfo\": \"A person contacted me through LinkedIn offering a job but asked for upfront fees, which seemed suspicious.\", \"category\": \"Scam\"},\n",
        "    {\"crimeaditionalinfo\": \"Received a suspicious email with a link to claim a refund from my bank. They requested my account and password details.\", \"category\": \"Phishing\"},\n",
        "    {\"crimeaditionalinfo\": \"A man is pretending to be my relative and is calling my office, asking for personal information. This is impacting my work.\", \"category\": \"Harassment\"},\n",
        "    {\"crimeaditionalinfo\": \"Received a fake message from someone posing as my friend, asking for money for an emergency.\", \"category\": \"Identity Theft\"}\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "\n",
        "class CustomTextDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_length, text_column='crimeaditionalinfo', label_column='category'):\n",
        "        self.data = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        self.data['encoded_label'] = self.label_encoder.fit_transform(self.data[label_column])\n",
        "        self.text_column = text_column\n",
        "        self.label_column = 'encoded_label'\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        text = self.data.iloc[idx][self.text_column]\n",
        "        label = self.data.iloc[idx][self.label_column]\n",
        "\n",
        "\n",
        "        tokens = self.tokenizer(\n",
        "            text,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        tokens = {key: val.squeeze(0) for key, val in tokens.items()}\n",
        "        return tokens, torch.tensor(label)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xFFLUIMu2J-a"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, tokenizer, texts, max_length=128):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    with torch.no_grad():\n",
        "        for text in texts:\n",
        "            tokens = tokenizer(\n",
        "                text,\n",
        "                padding='max_length',\n",
        "                max_length=max_length,\n",
        "                truncation=True,\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "            tokens = {key: val.to(model.device) for key, val in tokens.items()}\n",
        "            output = model(**tokens)\n",
        "            _, predicted_class = torch.max(output.logits, dim=1)\n",
        "            predictions.append(predicted_class.item())\n",
        "    return predictions\n",
        "\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "unique_labels = df['category'].nunique()\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=unique_labels)\n",
        "\n",
        "MAX_LENGTH = 128\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 10\n",
        "\n",
        "\n",
        "train_dataset = CustomTextDataset(df, tokenizer, MAX_LENGTH, text_column='crimeaditionalinfo', label_column='category')\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=4e-5)\n",
        "\n",
        "\n",
        "model.train()\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
        "    total_loss = 0\n",
        "    for batch in tqdm(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        tokens, labels = batch\n",
        "        outputs = model(**tokens, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    average_loss = total_loss / len(train_loader)\n",
        "    print(f\"Average Loss: {average_loss:.4f}\")\n",
        "\n",
        "\n",
        "model.save_pretrained(\"trained_bert_model\")\n",
        "tokenizer.save_pretrained(\"trained_bert_tokenizer\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6VnKz5DpTRy",
        "outputId": "c8a197a4-9bc6-4d1b-9b8b-da24220dae9c"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2/2 [00:47<00:00, 23.82s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Loss: 1.6199\n",
            "Epoch 2/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2/2 [00:25<00:00, 12.82s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Loss: 1.5648\n",
            "Epoch 3/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2/2 [00:25<00:00, 12.64s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Loss: 1.5390\n",
            "Epoch 4/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2/2 [00:25<00:00, 12.58s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Loss: 1.5358\n",
            "Epoch 5/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2/2 [00:25<00:00, 12.77s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Loss: 1.4448\n",
            "Epoch 6/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2/2 [00:24<00:00, 12.48s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Loss: 1.3267\n",
            "Epoch 7/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2/2 [00:24<00:00, 12.19s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Loss: 1.2326\n",
            "Epoch 8/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2/2 [00:24<00:00, 12.17s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Loss: 1.1382\n",
            "Epoch 9/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2/2 [00:23<00:00, 11.68s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Loss: 1.0894\n",
            "Epoch 10/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2/2 [00:23<00:00, 11.88s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Loss: 0.9585\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('trained_bert_tokenizer/tokenizer_config.json',\n",
              " 'trained_bert_tokenizer/special_tokens_map.json',\n",
              " 'trained_bert_tokenizer/vocab.txt',\n",
              " 'trained_bert_tokenizer/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "unknown_texts = [\n",
        "    \"someone posing as my friend, asking for money for an emergency. \"\n",
        "]\n",
        "predicted_classes = predict(model, tokenizer, unknown_texts)\n",
        "\n",
        "\n",
        "for text, prediction in zip(unknown_texts, predicted_classes):\n",
        "    print(f\"Text: {text} | Predicted Class: {prediction} | Class Label: {train_dataset.label_encoder.inverse_transform([prediction])[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHxr_9uXELEZ",
        "outputId": "3d0cc6c3-550c-4e02-c48d-25061a00bfbb"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: someone posing as my friend, asking for money for an emergency.  | Predicted Class: 2 | Class Label: Identity Theft\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wxzy7JRArIbN"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "\n",
        "data = {\n",
        "    'category': [\n",
        "        \"Online and Social Media Related Crime\", \"Online Financial Fraud\", \"Online Gambling Betting\",\n",
        "        \"Online and Social Media Related Crime\", \"Online Financial Fraud\", \"Online Financial Fraud\",\n",
        "        \"Online Financial Fraud\", \"Online Financial Fraud\", \"RapeGang Rape RGRSexually Abusive Content\",\n",
        "        \"Any Other Cyber Crime\", \"Online and Social Media Related Crime\", \"Any Other Cyber Crime\",\n",
        "        \"Online and Social Media Related Crime\", \"Online Financial Fraud\", \"Online Financial Fraud\",\n",
        "        \"Online Financial Fraud\", \"Online Financial Fraud\", \"Online Financial Fraud\", \"Online Financial Fraud\",\n",
        "        \"Cyber Attack/ Dependent Crimes\", \"Online Financial Fraud\", \"Online Gambling Betting\",\n",
        "        \"Online and Social Media Related Crime\", \"Cyber Attack/ Dependent Crimes\"\n",
        "    ],\n",
        "    'sub_category': [\n",
        "        \"Cyber Bullying Stalking Sexting\", \"Fraud CallVishing\", \"Online Gambling Betting\",\n",
        "        \"Online Job Fraud\", \"Fraud CallVishing\", \"UPI Related Frauds\", \"Fraud CallVishing\",\n",
        "        \"Internet Banking Related Fraud\", \"Sexually Abusive Content\", \"Other\", \"Cyber Bullying Stalking Sexting\",\n",
        "        \"Other\", \"Profile Hacking Identity Theft\", \"DebitCredit Card FraudSim Swap Fraud\",\n",
        "        \"UPI Related Frauds\", \"UPI Related Frauds\", \"UPI Related Frauds\", \"EWallet Related Fraud\",\n",
        "        \"Internet Banking Related Fraud\", \"Data Breach/Theft\", \"Fraud CallVishing\", \"Online Gambling Betting\",\n",
        "        \"Cheating by Impersonation\", \"Denial of Service (DoS)/Distributed Denial of Service (DDOS) attacks\"\n",
        "    ]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['sub_category'], df['category'], test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "tfidf = TfidfVectorizer()\n",
        "X_train_tfidf = tfidf.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf.transform(X_test)\n",
        "\n",
        "\n",
        "classifier = RandomForestClassifier(random_state=42)\n",
        "classifier.fit(X_train_tfidf, y_train)\n",
        "\n",
        "\n",
        "y_pred = classifier.predict(X_test_tfidf)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "\n",
        "sample_sub_category = [\"Fraud CallVishing\"]\n",
        "sample_tfidf = tfidf.transform(sample_sub_category)\n",
        "predicted_category = classifier.predict(sample_tfidf)\n",
        "print(\"\\nPredicted Category:\", predicted_category[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEbR4l7qrIvV",
        "outputId": "7da5bcd4-58ff-4d8c-f382-fdb8fbf1b5d0"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8\n",
            "\n",
            "Classification Report:\n",
            "                                            precision    recall  f1-score   support\n",
            "\n",
            "                    Any Other Cyber Crime       1.00      1.00      1.00         1\n",
            "                   Online Financial Fraud       0.67      1.00      0.80         2\n",
            "    Online and Social Media Related Crime       1.00      1.00      1.00         1\n",
            "RapeGang Rape RGRSexually Abusive Content       0.00      0.00      0.00         1\n",
            "\n",
            "                                 accuracy                           0.80         5\n",
            "                                macro avg       0.67      0.75      0.70         5\n",
            "                             weighted avg       0.67      0.80      0.72         5\n",
            "\n",
            "\n",
            "Predicted Category: Online Financial Fraud\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l01ysN6Jxic-"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text Filtering\n"
      ],
      "metadata": {
        "id": "Qy-bzam4xjw-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding, SpatialDropout1D\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "sentences = [\n",
        "    \"I received a call from someone asking for my bank account details.\",\n",
        "    \"You've won a lottery! Send us your bank details to claim the prize.\",\n",
        "    \"Please update your account information to prevent deactivation.\",\n",
        "    \"Hey, just wanted to check in on our meeting next week.\",\n",
        "    \"I need help with my order; it didn't arrive on time.\",\n",
        "    \"Your account has been compromised; please send your password to fix it.\",\n",
        "\n",
        "]\n",
        "labels = [1, 1, 1, 0, 0, 1]\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer(num_words=5000, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "padded_sequences = pad_sequences(sequences, maxlen=20, padding='post', truncating='post')\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=5000, output_dim=64, input_length=20),\n",
        "    SpatialDropout1D(0.2),\n",
        "    LSTM(64, dropout=0.2, recurrent_dropout=0.2),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, np.array(y_train), epochs=20, batch_size=1, validation_data=(X_test, np.array(y_test)))\n",
        "\n",
        "loss, accuracy = model.evaluate(X_test, np.array(y_test))\n",
        "print(f\"Test Accuracy: {accuracy:.2f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOj1oC7Hxm-T",
        "outputId": "6803c4e9-0fb6-43f5-a733-beb37bb15458"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 268ms/step - accuracy: 0.1667 - loss: 0.7028 - val_accuracy: 1.0000 - val_loss: 0.6705\n",
            "Epoch 2/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.2667 - loss: 0.7021 - val_accuracy: 1.0000 - val_loss: 0.6820\n",
            "Epoch 3/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.7333 - loss: 0.6845 - val_accuracy: 1.0000 - val_loss: 0.6720\n",
            "Epoch 4/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.4333 - loss: 0.6982 - val_accuracy: 1.0000 - val_loss: 0.6758\n",
            "Epoch 5/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8333 - loss: 0.6803 - val_accuracy: 1.0000 - val_loss: 0.6817\n",
            "Epoch 6/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.3667 - loss: 0.6964 - val_accuracy: 1.0000 - val_loss: 0.6861\n",
            "Epoch 7/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.3667 - loss: 0.6890 - val_accuracy: 1.0000 - val_loss: 0.6892\n",
            "Epoch 8/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.6852 - val_accuracy: 0.0000e+00 - val_loss: 0.6958\n",
            "Epoch 9/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.5333 - loss: 0.6849 - val_accuracy: 1.0000 - val_loss: 0.6848\n",
            "Epoch 10/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.6601 - val_accuracy: 1.0000 - val_loss: 0.6879\n",
            "Epoch 11/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 0.6314 - val_accuracy: 1.0000 - val_loss: 0.6813\n",
            "Epoch 12/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.6188 - val_accuracy: 0.5000 - val_loss: 0.6858\n",
            "Epoch 13/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 1.0000 - loss: 0.4969 - val_accuracy: 0.5000 - val_loss: 0.6429\n",
            "Epoch 14/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 1.0000 - loss: 0.3227 - val_accuracy: 0.5000 - val_loss: 0.5619\n",
            "Epoch 15/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 0.1318 - val_accuracy: 0.5000 - val_loss: 0.6257\n",
            "Epoch 16/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 1.0000 - loss: 0.0678 - val_accuracy: 0.5000 - val_loss: 0.9567\n",
            "Epoch 17/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 1.0000 - loss: 0.0396 - val_accuracy: 0.5000 - val_loss: 1.3765\n",
            "Epoch 18/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 0.0301 - val_accuracy: 0.5000 - val_loss: 1.6995\n",
            "Epoch 19/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 1.0000 - loss: 0.0207 - val_accuracy: 0.5000 - val_loss: 1.9570\n",
            "Epoch 20/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 1.0000 - loss: 0.0137 - val_accuracy: 0.5000 - val_loss: 2.1545\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 458ms/step - accuracy: 0.5000 - loss: 2.1545\n",
            "Test Accuracy: 0.50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_sentence = [\"Yourrrrrrrrr\"]\n",
        "new_sequence = tokenizer.texts_to_sequences(new_sentence)\n",
        "new_padded = pad_sequences(new_sequence, maxlen=20, padding='post', truncating='post')\n",
        "prediction = model.predict(new_padded)\n",
        "print(\"Fraud probability:\", prediction[0][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qh4maJPo6guE",
        "outputId": "841e0ad1-505e-4825-c0c0-3dea77a8fb79"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 290ms/step\n",
            "Fraud probability: 0.9886178\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YOcGDCIWKYP0"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cG3i5MrqKYXp"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RBGpY07BKYeG"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W6hALMS8KYki"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install transformers torch scikit-learn\n",
        "!pip install datasets\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZntH6sCKYnR",
        "outputId": "ca03ff6c-145e-4e7e-b302-6bc525346af6"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.101)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.10/dist-packages (from torch) (10.2.10.91)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.0.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.4.91)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.10/dist-packages (from torch) (2.14.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.91)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (75.1.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.44.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.30.5)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (18.1.8)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
            "Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.1.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from datasets import Dataset\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "sentences = [\n",
        "    \"I received a call from someone asking for my bank account details.\",  # fraud\n",
        "    \"You've won a lottery! Send us your bank details to claim the prize.\",  # fraud\n",
        "    \"Please update your account information to prevent deactivation.\",  # fraud\n",
        "    \"Hey, just wanted to check in on our meeting next week.\",  # not fraud\n",
        "    \"I need help with my order; it didn't arrive on time.\",  # not fraud\n",
        "    \"Your account has been compromised; please send your password to fix it.\",  # fraud\n",
        "    \"Urgent: Your bank account is under review. Kindly verify your personal information immediately to avoid restrictions.\",  # fraud\n",
        "    \"Warning: Unauthorized login attempts detected on your account. Please confirm your identity to secure your account.\",  # fraud\n",
        "    \"We noticed suspicious activity in your account. Please reply with your account number to verify your identity.\",  # fraud\n",
        "    \"You have been selected to receive a special reward. Please send your payment details to claim your prize.\",  # fraud\n",
        "    \"Your subscription is about to expire. To prevent service interruption, please update your payment information as soon as possible.\",  # not fraud\n",
        "    \"Important: Your account has been locked due to multiple failed login attempts. Click here to reset your password.\",  # fraud\n",
        "    \"We are conducting a security check. Kindly provide your social security number and date of birth to verify your account.\",  # fraud\n",
        "    \"Exclusive Offer: You've won a free vacation! Please provide your payment details to confirm your booking.\",  # fraud\n",
        "    \"We need to verify your identity. Please send a copy of your ID and recent utility bill to proceed.\",  # fraud\n",
        "    \"Congratulations! You've won a gift card worth $500. Please reply with your email address and payment info to claim it.\",  # fraud\n",
        "    \"Immediate action required: Your account has been flagged for suspicious activity. Please log in and verify your account details immediately.\",  # fraud\n",
        "\n",
        "]\n",
        "\n",
        "labels = [1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,1,1,1,1]\n",
        "\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(sentences, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize the input data\n",
        "train_encodings = tokenizer(X_train, truncation=True, padding=True, max_length=32)\n",
        "test_encodings = tokenizer(X_test, truncation=True, padding=True, max_length=32)\n",
        "\n",
        "# Create a Dataset object for the Hugging Face Trainer API\n",
        "train_dataset = Dataset.from_dict({'input_ids': train_encodings['input_ids'], 'attention_mask': train_encodings['attention_mask'], 'label': y_train})\n",
        "test_dataset = Dataset.from_dict({'input_ids': test_encodings['input_ids'], 'attention_mask': test_encodings['attention_mask'], 'label': y_test})\n",
        "\n",
        "# Load the pre-trained BERT model for sequence classification\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
        "\n",
        "\n",
        "\n",
        "# Define the compute_metrics function to calculate accuracy\n",
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    # Convert numpy.ndarray to torch.Tensor\n",
        "    predictions = torch.tensor(predictions)\n",
        "    # Apply argmax to get the predicted labels\n",
        "    preds = torch.argmax(predictions, dim=-1)\n",
        "    return {'accuracy': accuracy_score(labels, preds)}\n",
        "\n",
        "\n",
        "# Define training arguments, disable wandb\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # output directory\n",
        "    num_train_epochs=10,              # number of training epochs\n",
        "    per_device_train_batch_size=4,   # batch size for training\n",
        "    per_device_eval_batch_size=4,   # batch size for evaluation\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    logging_dir='./logs',\n",
        "    # learning_rate=5e-5,# directory for storing logs\n",
        "    report_to=\"none\",                # Disable Wandb\n",
        ")\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,                         # the model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=test_dataset,           # evaluation dataset\n",
        "    compute_metrics=compute_metrics      # pass the compute_metrics function\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate the model\n",
        "results = trainer.evaluate()\n",
        "print(f\"Test Accuracy: {results['eval_accuracy']:.2f}\")\n",
        "\n",
        "# Test with a new sentence\n",
        "new_sentence = \"Hey, just wanted to check in on our meeting next week.\"\n",
        "inputs = tokenizer(new_sentence, return_tensors=\"pt\", truncation=True, padding=True, max_length=32)\n",
        "output = model(**inputs)\n",
        "prediction = torch.argmax(output.logits, dim=-1).item()\n",
        "print(f\"Fraud prediction for new sentence: {'fraud' if prediction == 1 else 'not fraud'}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "51V6oqYzKYps",
        "outputId": "61638f2a-57f4-4fff-bd57-9cc5d9bd9bb4"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [40/40 02:15, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1/1 : < :]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 1.00\n",
            "Fraud prediction for new sentence: not fraud\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Test with a new sentence\n",
        "new_sentence = \"I received a call from someone asking for my bank account details.\"\n",
        "inputs = tokenizer(new_sentence, return_tensors=\"pt\", truncation=True, padding=True, max_length=32)\n",
        "output = model(**inputs)\n",
        "prediction = torch.argmax(output.logits, dim=-1).item()\n",
        "print(f\"Fraud prediction for new sentence: {'fraud' if prediction == 1 else 'not fraud'}\")"
      ],
      "metadata": {
        "id": "NhdSJua0Mb0L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a9b0b3c-8f3d-44a9-eafc-5997dfdd08b4"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fraud prediction for new sentence: fraud\n"
          ]
        }
      ]
    }
  ]
}